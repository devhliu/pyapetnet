{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charming-focus",
   "metadata": {},
   "source": [
    "# Training a convolutional network for anatomy guided PET image denoising and deblurring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-polyester",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to set up and train a simple 3D convolutional network for anatomical-guided denoising and deblurring of PET images.\n",
    "\n",
    "We will set up a network that takes a batch of 3D tensors with 2 channels (e.g. PET and MR) as input and outputs a batch of 3D tensors with 1 channel (denoised and deblurred PET image).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-phoenix",
   "metadata": {},
   "source": [
    "The approach and the model architecture that we will use in this tutorial is inspired by Schramm et al., [\"Approximating anatomically-guided PET reconstruction in image space using a convolutional neural network\"](https://doi.org/10.1016/j.neuroimage.2020.117399), NeuroImage 2021, DOI 10.1016/j.neuroimage.2020.117399\n",
    "\n",
    "![foo bar](https://raw.githubusercontent.com/gschramm/pyapetnet/master/figures/fig_1_apetnet.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-london",
   "metadata": {},
   "source": [
    "This tutotial uses simulated PET/MR data based on the brain web. However, applying the same training strategy to real data should be straight forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-adelaide",
   "metadata": {},
   "source": [
    "To setup and train the model, we will use tensorflow and keras to show the basic concepts of setting up and training a model. Of course, the same concepts can be used with any other deep learning frame work (such as e.g. pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-evaluation",
   "metadata": {},
   "source": [
    "**The tutorial is split into two notebooks:**\n",
    "1. In the first notebook ([01_tf_data.ipynb](01_tf_data.ipynb)), we will learn how to setup an data loader pipeline to efficiently create mini batches of training data including data augmentation.\n",
    "2. In the second notebook ([02_tf_models.ipynb](02_tf_models.ipynb)), we will learn how to setup and train the model architecture shown above.\n",
    "\n",
    "Finally, it wil be you turn to combine the knowledge of these two notebooks to train your own network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-israel",
   "metadata": {},
   "source": [
    "To run the tutorial, we recommend to install the python package:\n",
    "- ```pyapetnet v.1.1``` (or later) \n",
    "\n",
    "This will install all depdencies that we need (e.g. ```tensorflow``` and ```nibabel```). \n",
    "\n",
    "In addition the following packages are needed (for better visualizations):\n",
    "- ```pydot``` \n",
    "- ```graphviz```\n",
    "- ```ipympl```\n",
    "\n",
    "All packages are available on pypi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-trauma",
   "metadata": {},
   "source": [
    "## Downloading data used in this tutorial\n",
    "\n",
    "Please execute the cell below to download the data that we need in this tutorial. The data is in total ca 10GB and the download should **take ca. 15min depending on your connection**. **You only have to run this cell once.** After the download has finished, the downloaded zip file will be extracted into the folder ```brainweb_petmr```. Finally, all available simulated subjects will be printed. If everything went fine, 20 subjects should be available.\n",
    "\n",
    "If the data folder ```brainweb_petmr``` is available, nothing will be downloaded and all available subjects will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import urllib.request \n",
    "import zipfile\n",
    "\n",
    "data_path = pathlib.Path('brainweb_petmr')\n",
    "\n",
    "if not data_path.exists():\n",
    "  zip_path = pathlib.Path('brainweb_petmr.zip')\n",
    "  \n",
    "  if not zip_path.exists():\n",
    "    print('Downloading data from zenodo')\n",
    "    urllib.request.urlretrieve('https://zenodo.org/record/4897350/files/brainweb_petmr.zip', 'brainweb_petmr.zip')\n",
    "    print('Finished download')\n",
    "\n",
    "  print('Unzipping data')\n",
    "  with zipfile.ZipFile(zip_path,'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "\n",
    "  # remove the temporary zip file\n",
    "  zip_path.unlink()\n",
    "\n",
    "# print all downloaded subjects\n",
    "for i, p in enumerate(sorted(list(data_path.glob('*')))):\n",
    "  print(f'{(i+1):02}', str(p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
